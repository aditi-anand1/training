20th June 2021, Day 0: Initial Setup
Goal: To set up initial tech stack.
	1. Jupyter notebook
Installations:
	1. Python (3.7.9)
	2. Pip (19.2.3)

Commands to Install Packages:
	pip install pandas
	pip install matplotlib
	python -m pip install matplotlib
	python -m pip install seaborn
	pip install sklearn
	pip install xgboost
	pip install lightgbm
	pip install notebook
	pip install numpy

Command to launch Jupyter Notebook:
	jupyter notebook
	
Day 2: Data preprocessing
		
		
		
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
#read_csv() # use to load or import csv file
my_dataset=pd.read_csv("C:\\Users\\Aditi Anand\\Desktop\Training Data.csv")
#print(my_dataset)

my_dataset.shape

my_dataset.isnull().sum()

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test=train_test_split(
    my_dataset.drop(labels=['clear_date'],axis=1),
    my_dataset['clear_date'],
    test_size=0.3,
    random_state=0)
X_train.shape , X_test.shape

data_t=X_train.T
data_t.duplicated().sum()

print(my_dataset.dtypes)

# changing the datatype to datetime

import datetime
my_dataset['document_create_date']=pd.to_datetime(my_dataset['document_create_date'])
my_dataset['clear_date']=pd.to_datetime(my_dataset['clear_date'])
my_dataset['due_in_date']=pd.to_datetime(my_dataset['due_in_date'])
my_dataset['baseline_create_date']=pd.to_datetime(my_dataset['baseline_create_date'])
my_dataset['posting_date']=pd.to_datetime(my_dataset['posting_date'])
my_dataset.dtypes

# to make the correlation graph to observe data
import seaborn as sns
plt.subplots(figsize =(15,8))
sns.heatmap(my_dataset.corr(),annot=True,cmap="PiYG")
plt.title("Correlations Among Features",fontsize = 20);

#dropping posting_id as it is not realted at all
my_dataset=my_dataset.drop(["posting_id"],axis=1)

plt.subplots(figsize =(15,8))
sns.heatmap(my_dataset.corr(),annot=True,cmap="PiYG")
plt.title("Correlations Among Features",fontsize = 20);

# deleting col which are useless for prediction process
my_dataset=my_dataset.drop(["business_code"],axis=1)
my_dataset=my_dataset.drop(["cust_number"],axis=1)

my_dataset=my_dataset.drop(["invoice_id"],axis=1)
my_dataset=my_dataset.drop(["document type"],axis=1)
my_dataset=my_dataset.drop(["posting_date"],axis=1)

# to check nullvalues rows but in thsi we have null value in clear_date only so no null value is present in the col other than it 
my_dataset.isnull().sum()

my_dataset.shape
#preprocessing of data done